{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# What are artificial neural networks\n",
    "\n",
    "- ANNs are information processing models inspired by the human brain.\n",
    "- The brain has over 100 billion neyrons communicating through electrical and chemical signals. \n",
    "- Neurons communicate with each other and help us see, think, and generate ideas.\n",
    "Human brain learnins by creatin connections amonth these neurons. \n",
    "- Humans learn from experience.\n",
    "- ANNs will learn in the same fashion, by showing several training samples over and over again 'epochs'. \n",
    "\n",
    "## ANNs Advantages\n",
    "\n",
    "- ANNs are capable of modeling relationships between a set of inputs and outputs. \n",
    "- \"A neural network with sufficient number of neurons can approximately model any continuous function with an acceptavle degree of accuracy.\n",
    "- Excellent generalization capability. \n",
    "\n",
    "## ANNs limitation\n",
    "- Black-box structure\n",
    "- Even though trained network can operate properly, the internal structure of the network often has no physical significance and very difficult to understand. \n",
    "\n",
    "## Plan of atack\n",
    "\n",
    "Single neuron(Perceptron) - > Two neuron model -> Multi-layer perception network -> Deep networks \n",
    "\n",
    "# Single Neuron Perceptron Model\n",
    "\n",
    "## Neuron mathematical model\n",
    "\n",
    "- The neuron collects signals from input channels names dendtrites, processes information in its nucleus, and then generates an output in a long thin branch called the axon. \n",
    "- Weights shows the strength of the particular node. \n",
    "- Inputs/Independendt variables -> Neuron -> Output(Binary, Categorical, Continious)\n",
    "- Bias allows to shift the activation function curve up or down.\n",
    "- Number of adjustable parameters = 4(3 weights and 1 bias)\n",
    "- Activation function F\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Activaction Functions\n",
    "\n",
    "## Sigmoing\n",
    "- Takes a number and sets it between 0 and 1.\n",
    "- Converts large negative numbers to 0 and large positive numbers to 1.\n",
    "- Generally used in output layer.\n",
    "\n",
    "# RELU(Rectified Linear Units)\n",
    "- If input < 0, output is 0 and if x > 0 the output is x.\n",
    "- RELU does not saturate so it avoids vanishing gradient problem. \n",
    "- It uses simple thresholding so it is computationally efficient. \n",
    "- Generally used in hidden layers.\n",
    "\n",
    "# HYPERBOLIC TANGENT ACTIVAION FUNCTION\n",
    "- \"Tanh\" is similar to sigmoid, converts numbers between -1 and 1.\n",
    "- Unlike sigmoid, tanh outputs are zero-centered(range -1 to 1).\n",
    "- Tanh suffers from vanishing gradient problem so it kills gradients when saturated.\n",
    "- In practice, tanh is preferable over sigmoid."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ANN training and dataset split\n",
    "\n",
    "## How do ANN train ?\n",
    "- Like humans, ANNs can learn (or be trained) from experience (or by example) using training algorithms that can establish relationships between input-output datasets.\n",
    "- During the learning(training) phase, ANNs can adaptively change their internal structure.\n",
    "- Several ANNs training algorithms with various accuracy, speed, computational complexity, and memory requirements. \n",
    "\n",
    "## Learning strategy\n",
    "\n",
    "1. Supervised learning\n",
    "- Used if these is large set of test data with known labels(output)\n",
    "- The learning algorithm evaluates output (i.e: makes predictions), compares output against the label, and ajust network and repeat.\n",
    "\n",
    "Example: Email Spam Detection\n",
    "\n",
    "Task: Classify emails as either \"spam\" or \"not spam.\"\n",
    "Data: A dataset of emails labeled as \"spam\" or \"not spam,\" including features like the frequency of certain words, the presence of attachments, etc.\n",
    "Process: The algorithm learns from this labeled dataset to predict the class of new, unlabeled emails.\n",
    "\n",
    "2. Unsupervised learning\n",
    "- Used with \"unlabeled\" data (not categorized)(ex: k-means clustering)\n",
    "- Since learning algorithm works with unlabeled data, there is no way to assess\n",
    "\n",
    "Example: Customer Segmentation\n",
    "\n",
    "Task: Group customers into distinct segments based on their purchasing behavior.\n",
    "Data: A dataset with customer information such as purchase history, frequency, and amount spent, but without predefined categories.\n",
    "Process: The algorithm identifies patterns and clusters in the data, such as grouping customers into segments like \"frequent buyers,\" \"occasional buyers,\" and \"non-buyers.\"\n",
    "\n",
    "3. Reinforced learning\n",
    "- Learning algorithm takes actions that maximizes some notion of cumulative reward. \n",
    "- Over time, the network learns to prefer the right kind of action and to avoid the wrong one. \n",
    "\n",
    "Example: Training a Robot to Navigate a Maze\n",
    "\n",
    "Task: Teach a robot to find the shortest path from the start to the goal in a maze.\n",
    "Data: The robot interacts with the maze environment, receiving rewards or penalties based on its actions (e.g., a reward for reaching the goal, a penalty for hitting walls).\n",
    "Process: The robot learns an optimal policy over time by exploring different actions and receiving feedback, gradually improving its ability to navigate the maze effectively.\n",
    "\n",
    "\n",
    "## How are training techniques assessed ?\n",
    "\n",
    "- Speed of convergence (how many epochs). The faster to minimize the output error, the better.\n",
    "- \"epoch\" refers to one complete pass through the entire training dataset by the learning algorithm.\n",
    "- How the Mean Squared error varies with time.\n",
    "- Generalization capability.\n",
    "- Ability to avoid premature convergence to local minimum problems. "
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
