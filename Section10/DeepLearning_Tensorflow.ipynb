{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Intro to Deep learning and Tensorflow\n",
    "\n",
    "- Deep learning: Artificial neural network with more than one hidden layer. \n",
    "- As you stack layers, they can extract high levels features: Board, shapes, forms.. \n",
    "\n",
    "## Tensorflow\n",
    "\n",
    "- It is not specifically for neural networks - It´s more generally an architecture for executing a graph of numerical operations.\n",
    "- Tensorflow can optimize the processing of the graph, and distribute its processing accross a network.\n",
    "- It can also distribue work across GPU´s: Can handle massive scale.\n",
    "- Runs on about anything\n",
    "- Highly efficient C++ code with easy to use Python API´s\n",
    "- Tensor is just a fancy name for an array or matrix of values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "source": [
    "# Building Deep Neural Netowkrs with Keras, Normalization, and One-Hot Encoding\n",
    "\n",
    "## Creating a neural network with tensorflow\n",
    "\n",
    "- Mathematical Insights:\n",
    "1. All those interconnected arrows multiplying weights can be thought of as a big matrix multiplication.\n",
    "2. The bias term can just be added onto the result of that matrix multiplication. \n",
    "\n",
    "- So in Tensorflow, we can define a layer of a neural network as: output = td.matmul(previous_layer, layer_weights) + layer_biases\n",
    "- By using Tensorflow directly we are kinda doing this the \"hard way\".\n",
    "\n",
    "## Keras to the rescue\n",
    "\n",
    "- Easy and fast prototyping \n",
    "1. Higher-level API for tensorflow\n",
    "2. Made for building deep neural nets\n",
    "4. scikit_learn integration\n",
    "5. Less to think about - which often yields better results without even trying.\n",
    "6. This is really important ! The faster you can experiment, the better your results. \n",
    "\n",
    "## Make sure your features are normalizes\n",
    "\n",
    "- Neural network usually work best if your input data is normalized.\n",
    "1. That is, 0 mean and unit variance\n",
    "2. The real goal is that every input feature is comparable in terms of magnitude\n",
    "\n",
    "- scikit_learn's StandardScaler can do this for you\n",
    "- Many data sets are normalized to begin with"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Sequential() # We can build the model one layer at the time\n",
    "model.add(Dense(64, activation =\"relu\", input_dim =20))\n",
    "model.add(Dense(64, activation =\"relu\"))\n",
    "model.add(Dense(10, activation =\"softmax\")) # Output layer\n",
    "sgd = SGD(lr = 0.01, decay = 1e-6, momentum = 0.9, nesterov = True)\n",
    "model.compile(loss = 'categorical_crossentropy', optimizer = sgd, metrics = ['accuracy'])\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ReLu Activation, and Precenting Overfitting with Dropout Regularzation\n",
    "\n",
    "## Activation Function\n",
    "\n",
    "- Step functions don´t work with gradient descent - there is no gradient!\n",
    "1. Mathematically, they have no useful derivative.\n",
    "\n",
    "- Alternatives:\n",
    "1. Logistic(Sigmoid) function\n",
    "2. Hyperbolic tangent function\n",
    "3. Exponentional linear unit (ELU)\n",
    "4. ReLu function(Rectified Linear Unit)\n",
    "\n",
    "- ReLu is common. Fast to compute and works well.\n",
    "1. Also: \"Leaky ReLu\", \"Noisy ReLU\"\n",
    "2. ELU can sometimes lead to faster learning though\n",
    "\n",
    "## Avoid Overfitting with regularization\n",
    "\n",
    "1. With thousands of weights to tune, overfitting is a problem\n",
    "2. Early stopping (when performance starts dropping)\n",
    "3. Regularization terms added to cost function during training\n",
    "4. Dropout - ignore say 50% of all neurons randomly at each training step\n",
    "- Woks surprisingly well\n",
    "- Forces your model to spread out its learning "
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
