{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Intro to Deep learning and Tensorflow\n",
    "\n",
    "- Deep learning: Artificial neural network with more than one hidden layer. \n",
    "- As you stack layers, they can extract high levels features: Board, shapes, forms.. \n",
    "\n",
    "## Tensorflow\n",
    "\n",
    "- It is not specifically for neural networks - It´s more generally an architecture for executing a graph of numerical operations.\n",
    "- Tensorflow can optimize the processing of the graph, and distribute its processing accross a network.\n",
    "- It can also distribue work across GPU´s: Can handle massive scale.\n",
    "- Runs on about anything\n",
    "- Highly efficient C++ code with easy to use Python API´s\n",
    "- Tensor is just a fancy name for an array or matrix of values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "source": [
    "# Building Deep Neural Netowkrs with Keras, Normalization, and One-Hot Encoding\n",
    "\n",
    "## Creating a neural network with tensorflow\n",
    "\n",
    "- Mathematical Insights:\n",
    "1. All those interconnected arrows multiplying weights can be thought of as a big matrix multiplication.\n",
    "2. The bias term can just be added onto the result of that matrix multiplication. \n",
    "\n",
    "- So in Tensorflow, we can define a layer of a neural network as: output = td.matmul(previous_layer, layer_weights) + layer_biases\n",
    "- By using Tensorflow directly we are kinda doing this the \"hard way\".\n",
    "\n",
    "## Keras to the rescue\n",
    "\n",
    "- Easy and fast prototyping \n",
    "1. Higher-level API for tensorflow\n",
    "2. Made for building deep neural nets\n",
    "4. scikit_learn integration\n",
    "5. Less to think about - which often yields better results without even trying.\n",
    "6. This is really important ! The faster you can experiment, the better your results. \n",
    "\n",
    "## Make sure your features are normalizes\n",
    "\n",
    "- Neural network usually work best if your input data is normalized.\n",
    "1. That is, 0 mean and unit variance\n",
    "2. The real goal is that every input feature is comparable in terms of magnitude\n",
    "\n",
    "- scikit_learn's StandardScaler can do this for you\n",
    "- Many data sets are normalized to begin with"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mRunning cells with 'Python 3.12.6' requires the ipykernel package.\n",
      "\u001b[1;31mRun the following command to install 'ipykernel' into the Python environment. \n",
      "\u001b[1;31mCommand: 'c:/Users/jgsnt/AppData/Local/Microsoft/WindowsApps/python3.12.exe -m pip install ipykernel -U --user --force-reinstall'"
     ]
    }
   ],
   "source": [
    "model = Sequential() # We can build the model one layer at the time\n",
    "model.add(Dense(64, activation =\"relu\", input_dim =20))\n",
    "model.add(Dense(64, activation =\"relu\"))\n",
    "model.add(Dense(10, activation =\"softmax\")) # Output layer\n",
    "sgd = SGD(lr = 0.01, decay = 1e-6, momentum = 0.9, nesterov = True)\n",
    "model.compile(loss = 'categorical_crossentropy', optimizer = sgd, metrics = ['accuracy'])\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ReLu Activation, and Precenting Overfitting with Dropout Regularzation\n",
    "\n",
    "## Activation Function\n",
    "\n",
    "- Step functions don´t work with gradient descent - there is no gradient!\n",
    "1. Mathematically, they have no useful derivative.\n",
    "\n",
    "- Alternatives:\n",
    "1. Logistic(Sigmoid) function\n",
    "2. Hyperbolic tangent function\n",
    "3. Exponentional linear unit (ELU)\n",
    "4. ReLu function(Rectified Linear Unit)\n",
    "\n",
    "- ReLu is common. Fast to compute and works well.\n",
    "1. Also: \"Leaky ReLu\", \"Noisy ReLU\"\n",
    "2. ELU can sometimes lead to faster learning though\n",
    "\n",
    "## Avoid Overfitting with regularization\n",
    "\n",
    "1. With thousands of weights to tune, overfitting is a problem\n",
    "2. Early stopping (when performance starts dropping)\n",
    "3. Regularization terms added to cost function during training\n",
    "4. Dropout - ignore say 50% of all neurons randomly at each training step\n",
    "- Woks surprisingly well\n",
    "- Forces your model to spread out its learning "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Max Pooling\n",
    "\n",
    "Max pooling is a down-sampling technique commonly used in convolutional neural networks (CNNs) for image processing and other applications. Its primary purpose is to reduce the spatial dimensions of the input feature maps while retaining the most significant features.\n",
    "\n",
    "- How Max Pooling Works:\n",
    "Input Feature Map: Max pooling operates on a feature map generated by a convolutional layer. This feature map contains activations that represent different features detected in the input.\n",
    "\n",
    "- Pooling Window: A pooling window (or filter) is defined, typically of size 2x2 or 3x3. This window slides over the input feature map.\n",
    "\n",
    "- Stride: The stride determines how far the window moves each time. A common stride value is 2, meaning the window shifts two pixels at a time.\n",
    "\n",
    "- Operation: For each position of the pooling window, max pooling takes the maximum value within that window and outputs it to the new, down-sampled feature map.\n",
    "\n",
    "- Output Feature Map: The result is a smaller feature map, which retains the most significant activations from the original. This helps reduce computational load and prevents overfitting.\n",
    "\n",
    "- Benefits of Max Pooling:\n",
    "Dimensionality Reduction: Reduces the number of parameters and computations in the network, making it more efficient.\n",
    "Feature Retention: By selecting the maximum values, max pooling preserves the most prominent features while discarding less significant details.\n",
    "Translation Invariance: Helps the model become more robust to small translations in the input, as the exact position of the features becomes less important.\n",
    "\n",
    "## Image value definition\n",
    "\n",
    "The value of each pixel in an image is defined based on the type of image and the color model being used. Here’s a breakdown of how pixel values are determined:\n",
    "\n",
    "1. Color Models\n",
    "Grayscale Images:\n",
    "-Each pixel value represents a shade of gray, typically ranging from 0 (black) to 255 (white) in an 8-bit image. The value indicates the intensity of light; higher values correspond to lighter shades.\n",
    "\n",
    "RGB Images:\n",
    "In an RGB (Red, Green, Blue) image, each pixel is defined by three values, one for each color channel:\n",
    "1. Red Channel: Intensity of red (0-255)\n",
    "2. Green Channel: Intensity of green (0-255)\n",
    "3. Blue Channel: Intensity of blue (0-255)\n",
    "The combination of these three channels produces the final color of the pixel.\n",
    "Other Color Models:\n",
    "\n",
    "Other models, like CMYK (Cyan, Magenta, Yellow, Key/Black) for printing, or HSV (Hue, Saturation, Value), have different ways to define color, but the principle is similar."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.12.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
