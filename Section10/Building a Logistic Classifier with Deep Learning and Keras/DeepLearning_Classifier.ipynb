{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as numpy\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib import _cmfrom sklearn.datasets import make_blobs\n",
    "from pylab import *\n",
    "from sklearn.model_selection import train_test_split\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_samples = 10000\n",
    "\n",
    "# To generate our fabricated data set, we will create two blobs of randomly distributed data at two corners of the graph\n",
    "centers = [(0.9, 0.05), (0.05, 0.94)]\n",
    "X, y = make_blobs(n_samples = n_samples, n_features = 2, cluster_std = 0.4, centers = centers, shuffle = False, random_sate = 42)\n",
    "\n",
    "# Then filter out only the values that lie within [0, 1]:\n",
    "features = []\n",
    "labels = []\n",
    "\n",
    "for feature, label in zip(X, y):\n",
    "    if (feature[0] >= 0 and feature[0] <= 1.0 and feature[1] >= 0 and feature[1] <= 1.0):\n",
    "        features.append(feature)\n",
    "        labels.append(label)\n",
    "        \n",
    "# And convert the results back into numpy arrays:\n",
    "X = np.array(features)\n",
    "y = np.array(labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize = (8,6))\n",
    "plt.scatter(X[:,0], X[:,1], c=y.astype(np.float))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state = 42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Sequential()\n",
    "model.add(Dense(32, activation = 'relu', input_shape=X_train[1:]))\n",
    "model.add(Dense(1, activation='sigmoid'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(loss='binary_crossentropy',\n",
    "              optimizer ='adam',\n",
    "              metrics =['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "history = model.fit(X_train, y_train,\n",
    "                    batch_size = 100,\n",
    "                    epochs = 10,\n",
    "                    verbose = 2,\n",
    "                    validation_data =(X_test, y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# To make sure we are not overfitting to the training data, we can easily evaluate it against the testing data\n",
    "scores = model.evaluate(X_test, y_test, verbose = 1)\n",
    "print('Test loss: ', scores[0])\n",
    "print('Test accuracy: ', scores[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Increasing the difficult. Dedining a data universe of four categories. \n",
    "centers = [(0.9, 0.05), (0.05, 0.94), (0.3, 0.6), (0.8, 0.8)]\n",
    "X, y = make_blobs(n_samples=n_samples, n_features=2, cluster_std=0.2,\n",
    "                  centers=centers, shuffle=False, random_state=42)\n",
    "\n",
    "# Then filter out only the values that lie within [0,1]:\n",
    "features = []\n",
    "labels = []\n",
    "for feature, label in zip(X, y):\n",
    "    if (feature[0] >= 0 and feature[0] <= 1.0 and feature[1] >= 0 and feature[1] <= 1.0):\n",
    "        features.append(feature)\n",
    "        labels.append(label)\n",
    "\n",
    "# And convert the results back into numpy arrays:\n",
    "X = np.array(features)\n",
    "y = np.array(labels)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(8, 6))\n",
    "plt.scatter(X[:,0], X[:,1], c=y.astype(np.float))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Defining the model\n",
    "# Now we have a 4 categories instead of 2, our final layer now has 4 units in it. \n",
    "model = Sequential()\n",
    "model.add(Dense(32, activation='relu', input_shape=X_train.shape[1:]))\n",
    "model.add(Dense(4, activation='softmax'))\n",
    "          \n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compile the model\n",
    "model.compile(loss='categorical_crossentropy',\n",
    "              optimizer='adam',\n",
    "              metrics=['accuracy'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train the model\n",
    "history = model.fit(X_train, y_train,\n",
    "                    batch_size=100,\n",
    "                    epochs=10,\n",
    "                    verbose=2,\n",
    "                    validation_data=(X_test, y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate the results\n",
    "scores = model.evaluate(X_test, y_test, verbose=1)\n",
    "print('Test loss:', scores[0])\n",
    "print('Test accuracy:', scores[1])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Let's see if we can improve our results by adding another layer of neurons, making this truly a \"deep learning\" example. All we're changing here is adding an extra layer of 16 units in between our first layer of 32 and our final layer of 4. The idea is to learn higher-level patterns in the data with that extra layer.\n",
    "'''\n",
    "\n",
    "model = Sequential()\n",
    "model.add(Dense(32, activation='relu', input_shape=X_train.shape[1:]))\n",
    "model.add(Dense(16, activation='relu'))\n",
    "model.add(Dense(4, activation='softmax'))\n",
    "\n",
    "model.compile(loss='categorical_crossentropy',\n",
    "              optimizer='adam',\n",
    "              metrics=['accuracy'])\n",
    "          \n",
    "print(model.summary())\n",
    "\n",
    "history = model.fit(X_train, y_train,\n",
    "                    batch_size=100,\n",
    "                    epochs=10,\n",
    "                    verbose=2,\n",
    "                    validation_data=(X_test, y_test))\n",
    "\n",
    "scores = model.evaluate(X_test, y_test, verbose=1)\n",
    "print('Test loss:', scores[0])\n",
    "print('Test accuracy:', scores[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Sequential()\n",
    "model.add(Dense(32, activation='relu', input_shape=X_train.shape[1:]))\n",
    "model.add(Dense(16, activation='relu'))\n",
    "model.add(Dense(8, activation='relu'))\n",
    "model.add(Dense(4, activation='softmax'))\n",
    "\n",
    "model.compile(loss='categorical_crossentropy',\n",
    "              optimizer='adam',\n",
    "              metrics=['accuracy'])\n",
    "          \n",
    "print(model.summary())\n",
    "\n",
    "history = model.fit(X_train, y_train,\n",
    "                    batch_size=100,\n",
    "                    epochs=10,\n",
    "                    verbose=2,\n",
    "                    validation_data=(X_test, y_test))\n",
    "\n",
    "scores = model.evaluate(X_test, y_test, verbose=1)\n",
    "print('Test loss:', scores[0])\n",
    "print('Test accuracy:', scores[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Well, that actually did more harm than good. That usually means we're starting to overfit the data; this network is probably more complex than it needs to be at this point.\n",
    "\n",
    "What if instead of going deep, we go wide? We can try going back to 2 layers, but just adding more neurons to that single hidden layer. Let's try doubling it from 32 to 64:\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Sequential()\n",
    "model.add(Dense(64, activation='relu', input_shape=X_train.shape[1:]))\n",
    "model.add(Dense(4, activation='softmax'))\n",
    "\n",
    "model.compile(loss='categorical_crossentropy',\n",
    "              optimizer='adam',\n",
    "              metrics=['accuracy'])\n",
    "          \n",
    "print(model.summary())\n",
    "\n",
    "history = model.fit(X_train, y_train,\n",
    "                    batch_size=100,\n",
    "                    epochs=10,\n",
    "                    verbose=2,\n",
    "                    validation_data=(X_test, y_test))\n",
    "\n",
    "scores = model.evaluate(X_test, y_test, verbose=1)\n",
    "print('Test loss:', scores[0])\n",
    "print('Test accuracy:', scores[1])"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
