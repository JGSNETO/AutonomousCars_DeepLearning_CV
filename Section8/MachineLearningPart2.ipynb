{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Bayes Theorem and Naive Bayes\n",
    "\n",
    "\\[ P(A|B) = \\frac{P(B|A) \\cdot P(A)}{P(B)} \\]\n",
    "\n",
    "Where:\n",
    "\n",
    "- \\( P(A|B) \\) is the probability of event A occurring given that B is true.\n",
    "- \\( P(B|A) \\) is the probability of event B occurring given that A is true.\n",
    "- \\( P(A) \\) is the probability of event A.\n",
    "- \\( P(B) \\) is the probability of event B.\n",
    "\n",
    "- The key insight is that the probability of something that depends on B depends very much on the base probability of B and A. People ignore this all the time. \n",
    "\n",
    "\n",
    "# Support vector machine\n",
    "\n",
    "- Works well for classifying higher-dimenstional data.\n",
    "- Finds higher-dimenstional support vectors across which to divide the data(Mathematically, these support vectors define hyperplanes. Needless to say IÂ´m not going to get into the mathematical details)\n",
    "- Uses something called the kernel-trick to represent data in higher-dimensional spaces to find hyperplanes that might not be apparant in lower dimensions. \n",
    "- The important point is that SVM's employ some advanced mathematical trickery to cluster data, and it can handle data sets with lots of features.\n",
    "- It is also fairly expensive - The \"kernel trick\" is the only think that makes it possible. \n",
    "\n",
    "## Support vector classification\n",
    "\n",
    "- In practice you will use something called SVC to classify data using SVM.\n",
    "- You can use different \"kernels\" with SVC. Some will work better than others for a given data set. \n",
    "\n",
    "## Avoinding overfitting\n",
    "\n",
    "- Overfitting refers to a model that has learned the training data too well, including its noise and outliers, to the extent that it performs poorly on new, unseen data. This typically happens when a model is too complex relative to the amount of training data, leading it to capture not just the underlying patterns but also the random fluctuations or noise in the data.\n",
    "- Dont choose an overly complex kernel.\n",
    "- RBF is very sensitive to gamma values chosen.\n",
    "- This specifies the area of influence of the support vectors\n",
    "- Use k-fold cross validation, GridSearchCV"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
