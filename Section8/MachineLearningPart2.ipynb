{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Bayes Theorem and Naive Bayes\n",
    "\n",
    "\\[ P(A|B) = \\frac{P(B|A) \\cdot P(A)}{P(B)} \\]\n",
    "\n",
    "Where:\n",
    "\n",
    "- \\( P(A|B) \\) is the probability of event A occurring given that B is true.\n",
    "- \\( P(B|A) \\) is the probability of event B occurring given that A is true.\n",
    "- \\( P(A) \\) is the probability of event A.\n",
    "- \\( P(B) \\) is the probability of event B.\n",
    "\n",
    "- The key insight is that the probability of something that depends on B depends very much on the base probability of B and A. People ignore this all the time. \n",
    "\n",
    "\n",
    "# Support vector machine\n",
    "\n",
    "- Works well for classifying higher-dimenstional data.\n",
    "- Finds higher-dimenstional support vectors across which to divide the data(Mathematically, these support vectors define hyperplanes. Needless to say IÂ´m not going to get into the mathematical details)\n",
    "- Uses something called the kernel-trick to represent data in higher-dimensional spaces to find hyperplanes that might not be apparant in lower dimensions. \n",
    "- The important point is that SVM's employ some advanced mathematical trickery to cluster data, and it can handle data sets with lots of features.\n",
    "- It is also fairly expensive - The \"kernel trick\" is the only think that makes it possible. \n",
    "\n",
    "A Support Vector Machine (SVM) is a powerful supervised machine learning algorithm primarily used for classification tasks, though it can also be applied to regression problems. The main idea behind SVMs is to find the optimal hyperplane that best separates data points of different classes in a high-dimensional space.\n",
    "\n",
    "## Key Concepts of Support Vector Machines\n",
    "Hyperplane:\n",
    "\n",
    "In SVM, a hyperplane is a decision boundary that separates data points of different classes. In a two-dimensional space, this hyperplane is simply a line, but in higher dimensions, it becomes a plane or a hyperplane.\n",
    "The goal of an SVM is to find the hyperplane that best separates the data points of different classes with the maximum margin.\n",
    "Margin:\n",
    "\n",
    "The margin is the distance between the hyperplane and the nearest data points of each class. These nearest points are known as support vectors.\n",
    "A large margin is desirable because it indicates that the classifier is more robust to variations and noise in the data. The SVM algorithm attempts to maximize this margin, which helps in achieving better generalization on unseen data.\n",
    "Support Vectors:\n",
    "\n",
    "Support vectors are the data points that lie closest to the hyperplane. These points are critical in defining the position and orientation of the hyperplane.\n",
    "They are called \"support vectors\" because they are the vectors (points) that are \"supporting\" the position of the hyperplane.\n",
    "\n",
    "## Support vector classification\n",
    "\n",
    "- In practice you will use something called SVC to classify data using SVM.\n",
    "- You can use different \"kernels\" with SVC. Some will work better than others for a given data set. \n",
    "\n",
    "Applications of SVM\n",
    "- Text and Hypertext Classification: SVMs are widely used in text classification tasks where the number of features (words) can be very high.\n",
    "- Image Classification: Due to their effectiveness in high-dimensional spaces, SVMs are frequently used for image classification tasks.\n",
    "- Bioinformatics: SVMs are used in various biological applications like protein classification and gene expression data analysis.\n",
    "\n",
    "## Parameters optimization\n",
    "\n",
    "- C parameter: Controls trade-off between classifying training points correctly and having a smooth decision boundery.\n",
    "1. Small C(loose) makes cost (penalty) of misclassification low (soft margin)\n",
    "2. Large C(strict) makes cost of misclassification high (hard margin), forcing the model to explain input data stricter and potentially over fit. \n",
    "\n",
    "- Gamma parameter: controls how far the influence of a single training set reachs\n",
    "1. Large gamma: Close reach(Closer data points have high weight)\n",
    "2. Small gamma: Far reach( More generalized solution)\n",
    "\n",
    "# Avoinding overfitting\n",
    "\n",
    "- Overfitting refers to a model that has learned the training data too well, including its noise and outliers, to the extent that it performs poorly on new, unseen data. This typically happens when a model is too complex relative to the amount of training data, leading it to capture not just the underlying patterns but also the random fluctuations or noise in the data.\n",
    "- Dont choose an overly complex kernel.\n",
    "- RBF is very sensitive to gamma values chosen.\n",
    "- This specifies the area of influence of the support vectors\n",
    "- Use k-fold cross validation, GridSearchCV"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
