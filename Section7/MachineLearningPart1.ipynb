{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# What is machine learning ?\n",
    "\n",
    "- Machine learning systems train a model by feeding it feature data with known labels.\n",
    "- Features are the attributes used to inform your model's predictions labels are the \"correct answers\" our model wants to be able to predict on its own.\n",
    "- Once the model is built, the model will predict labels just given features data.\n",
    "- Features are independent variables labels are dependent variabels.\n",
    "\n",
    "## how do machine learning model works ?\n",
    "\n",
    "- Many different approaches, but genereally speaking they tweak parameters of some mathemactical model to minimize prediction errors as they are trained with new data.\n",
    "- Neural networks, or \"deep learning\", are just the latest approach, and are one of many machine learning techiniques.\n",
    "- Other include reinforcement learning, support vector machines, and decision trees.\n",
    "\n",
    "# Evaluating machine learning systems\n",
    "\n",
    "*Train/Test*\n",
    "\n",
    "- If you have a set of training data that includes the value you are trying to predict - you don't have to guess if they resulting model is good or not.\n",
    "- If you have enough training data, you can split it into two parts: A training set and a test set.\n",
    "- You then train the model using only the traning set.\n",
    "- And then measure(using r-squared or some other metric) the model's accuracy by ask it to predict values for the test set, and compare that to the known, true values.\n",
    "\n",
    "## Train/set in practice\n",
    "\n",
    "- Need to ensure both sets are large enough to contain representatives of all the variantions and outliers in the data you care about.\n",
    "- The date sets must be selected randomly.\n",
    "- Train/test is a great way to guard against overfitting.\n",
    "\n",
    "## Train/Test is not infallible\n",
    "\n",
    "- Maybe your sample sizes are too small.\n",
    "- Or due to random chance your train and test sets look remarkably similar.\n",
    "- Overfitting can still happen.\n",
    "\n",
    "# K-fold cross validation\n",
    "\n",
    "- One way to further protect against overfitting is K-fold cross validation\n",
    "- Sounds complicated, but it's a simple idea:\n",
    "1. Split your data into K randomly-assigned segments\n",
    "2. Reserve one segment as your test data\n",
    "3. Train on each of the remaining k-1 segments and measure their performance against the test set.\n",
    "4. Take the average of the k-1 r-squared scores.\n",
    "  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "# ML vs DL vs RL\n",
    "\n",
    "## Machine Learning (ML):\n",
    "\n",
    "Definition: Machine Learning is a subset of artificial intelligence that involves training algorithms to recognize patterns and make decisions based on data. Instead of being explicitly programmed to perform a task, a machine learning model learns from examples.\n",
    "Types:\n",
    "- Supervised Learning: The model is trained on labeled data, meaning the input comes with the correct output. The goal is to predict the output for new, unseen data. Examples include regression and classification tasks.\n",
    "Unsupervised Learning: The model is trained on unlabeled data and tries to find patterns or structure in the data. Examples include clustering and dimensionality reduction.\n",
    "- Semi-Supervised Learning: Uses a combination of labeled and unlabeled data.\n",
    "- Self-Supervised Learning: The model generates its own labels from the data to train itself.\n",
    "Applications: Spam detection, recommendation systems, fraud detection, etc.\n",
    "Deep Learning:\n",
    "\n",
    "Definition: \n",
    "- Deep Learning is a specialized subset of machine learning that involves neural networks with many layers (hence \"deep\"). These networks can learn to represent data through multiple levels of abstraction, which allows them to handle more complex tasks.\n",
    "Architecture:\n",
    "- Neural Networks: Models composed of layers of interconnected nodes (neurons). Each layer transforms the input data in a way that contributes to the final output.\n",
    "- Convolutional Neural Networks (CNNs): Particularly good at processing grid-like data such as images.\n",
    "- Recurrent Neural Networks (RNNs): Suitable for sequential data like time series or natural language.\n",
    "Applications: Image recognition, natural language processing, voice recognition, etc.\n",
    "Reinforcement Learning (RL):\n",
    "\n",
    "Definition: \n",
    "Reinforcement Learning is a type of machine learning where an agent learns to make decisions by taking actions in an environment to maximize cumulative rewards. Itâ€™s inspired by behavioral psychology and is used when the goal is to learn a policy that maps states of the environment to actions.\n",
    "Key Concepts:\n",
    "- Agent: The learner or decision maker.\n",
    "- Environment: The world the agent interacts with.\n",
    "- Action: Choices made by the agent that affect the environment.\n",
    "- Reward: Feedback from the environment based on the action taken.\n",
    "- Policy: A strategy that the agent uses to determine actions based on states.\n",
    "- Applications: Game playing (like AlphaGo), robotics, autonomous vehicles, etc."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Linear Regression\n",
    "\n",
    "- Fit a line to a data set of observations.\n",
    "- Use this line to predict unobserved values.\n",
    "- The term \"regression\" is confusing.\n",
    "\n",
    "## how it does work ?\n",
    "\n",
    "- Usually using \"ordinary\" least squares.\n",
    "- Minimizes the squared-error between each point and the line.\n",
    "- Remember the slope-intercept equations of a line ?\n",
    "- remember the slope-intercept equation of a line ? y =mx+b\n",
    "- The slope is the correlations between the two variables times the standard deviation in Y, all divided by the standard deviation in X.\n",
    "- Neat how standard deviation how some real mathematical meaning, eh ?\n",
    "- The intercept is the mean of Y minus the slope times the mean of X.\n",
    "- But python will do all that for you.\n",
    "- Least squares minimizes the sum of squared errors.\n",
    "- This is the same as maximizing the likelihood of the observed data if you start thinking of the problem in terms of probabilitites and probability distribution functions.\n",
    "- Thi is sometimes called \"Maximum likeliheood estimation\".\n",
    "- Gradient descent is an alternative method to ordinary least squares.\n",
    "- Basically iterates to find the line that best follows the contours defined by the data.\n",
    "- Can make sense when dealing with multi-dimesional data.\n",
    "- Popular in neural networks.\n",
    "\n",
    "## Measuring error with r-squared\n",
    "\n",
    "- R-squared measures: The fraction of the total variation in Y that is captured by the model.\n",
    "- Ranges from 0-1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Logistic Regression\n",
    "\n",
    "- Regression for binary(yes/no) problems.\n",
    "- Like linear regression, but final predictions are transformed into a 0-1 range.\n",
    "- Least-squares won't work with the logistic curve.\n",
    "- Instead we use gradient descent with maximum likelihood.\n",
    "- This just iterates to maximize the conditional probability of Y given X."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Decision Tree & Random Forest\n",
    "\n",
    "- You can actually construct a flowchart to help you decide a classification for something with machine learning.\n",
    "- Works with numbers. \n",
    "- This is called a decision tree.\n",
    "- Another form of supervised learning.\n",
    "1. Give it some sample data and the resulting classifications.\n",
    "2. Out comes a tree. \n",
    "\n",
    "## How it works\n",
    "\n",
    "- At each step, find the attribute we can use to partition the data set to minimize the entropy of the data at the next step.\n",
    "- Fancy term for this simple algorithm: ID3\n",
    "- It is a greedy algorithm - as it goes down the tree, it just picks the decision that reduce entropy the most at that stage.\n",
    "1. That might not actually result in an optimal tree.\n",
    "2. But it works. \n",
    "\n",
    "## Random forests\n",
    "\n",
    "- Decision trees are very susceptible to overfitting.\n",
    "- To fight this, we can construct several alternative decision trees and let them \"vote\" on the final classification.\n",
    "1. Randomly re-sample the input data for each tree(Booststrap aggregating or bagging).\n",
    "2. Randomize a subset of the attributes each step is allowed to choose from. "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
